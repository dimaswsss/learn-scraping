{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIJOhpu7CFG8YJpAsfIo9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimaswsss/learn-scraping/blob/main/learn_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YSBqRzzEQSRs"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.scrapethissite.com/pages/simple/'"
      ],
      "metadata": {
        "id": "uCa2rV3TQchj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data  = requests.get(url).text"
      ],
      "metadata": {
        "id": "eDAuP8iDSaq1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(data,\"html5lib\")  # create a soup object using the variable 'data'"
      ],
      "metadata": {
        "id": "otSfhUzhQXtE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get title\n",
        "'''\n",
        "<h1>\n",
        "                            Countries of the World: A Simple Example\n",
        "                            <small>250 items</small>\n",
        "                        </h1>\n",
        "'''\n",
        "for link in soup.find_all('h1'):\n",
        "    title_text = link.contents[0].strip()\n",
        "    print(title_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtdfPgZ-W1VH",
        "outputId": "e5fabd4c-31b3-4a0a-eb99-ae1f8d827344"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries of the World: A Simple Example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get total data counries\n",
        "'''\n",
        "<h1>\n",
        "                            Countries of the World: A Simple Example\n",
        "                            <small>250 items</small>\n",
        "                        </h1>\n",
        "'''\n",
        "for link in soup.find_all('h1'):\n",
        "    # title_text = link.contents[1].strip()\n",
        "    # link.find_all('span', class_='country-capital')\n",
        "    print(link.find('small').text)\n",
        "    # print(link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufjjn0bgXZlA",
        "outputId": "9c82ba84-8c69-4409-9b23-5e4ea7c2c098"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250 items\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get desc\n",
        "'''\n",
        "<p class=\"lead\">\n",
        "                            A single page that lists information about all the countries in the world. Good for those just get started with web scraping.\n",
        "                            Practice looking for patterns in the HTML that will allow you to extract information about each country. Then, build a simple web scraper that makes a request to this page, parses the HTML and prints out each country's name.\n",
        "                        </p>\n",
        "'''\n",
        "description = soup.find('p', class_='lead').text.strip()\n",
        "description = ' '.join(description.split())\n",
        "print(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnMZ8ub3X8sv",
        "outputId": "d17ea319-c324-41b3-ff67-c4628ee000af"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single page that lists information about all the countries in the world. Good for those just get started with web scraping. Practice looking for patterns in the HTML that will allow you to extract information about each country. Then, build a simple web scraper that makes a request to this page, parses the HTML and prints out each country's name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get source\n",
        "'''\n",
        "<p>\n",
        "\n",
        "                                Data via\n",
        "                                <a href=\"http://peric.github.io/GetCountries/\" class=\"data-attribution\" target=\"_blank\">http://peric.github.io/GetCountries/</a>\n",
        "\n",
        "                        </p>\n",
        "'''\n",
        "for p_tag in soup.find_all('p'):\n",
        "    if p_tag.find('a'):\n",
        "        source_url = p_tag.find('a')['href']\n",
        "        print(source_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RhYUB4ZIFF",
        "outputId": "147b7eae-873d-4ed2-bcee-852f9cfe12d4"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/lessons/\n",
            "http://peric.github.io/GetCountries/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "country_names = []  # Create an empty list to store the country names\n",
        "\n",
        "for link in soup.find_all('h3'):\n",
        "    country_name = link.text.strip()\n",
        "    country_names.append(country_name)  # Add each country name to the list\n",
        "\n",
        "top_5_names = country_names[:5]  # Get the top 5 names from the list\n",
        "\n",
        "for name in top_5_names:\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CPhuMwQSjub",
        "outputId": "d334caac-809b-432e-cd7a-a7647171b59c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Andorra\n",
            "United Arab Emirates\n",
            "Afghanistan\n",
            "Antigua and Barbuda\n",
            "Anguilla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capital_dict = {}  # Create an empty dictionary to store capital names and their frequencies\n",
        "\n",
        "for link in soup.find_all('div'):\n",
        "    cols = link.find_all('span', class_='country-capital')\n",
        "    for col in cols:\n",
        "        capital_name = col.text.strip()\n",
        "        if capital_name in capital_dict:\n",
        "            capital_dict[capital_name] += 1  # Increment the frequency of the capital name\n",
        "        else:\n",
        "            capital_dict[capital_name] = 1  # Add the capital name to the dictionary\n",
        "\n",
        "top_5_capitals = sorted(capital_dict, key=capital_dict.get, reverse=True)[:5]  # Sort the dictionary by frequency and get the top 5 capitals\n",
        "\n",
        "for capital in top_5_capitals:\n",
        "    print(capital)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQhHRUK2TD9Z",
        "outputId": "ca91bf72-d71f-494b-c0cf-e97404ded761"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Kingston\n",
            "Andorra la Vella\n",
            "Abu Dhabi\n",
            "Kabul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capital_set = set()  # Membuat set kosong untuk menyimpan nama ibu kota yang unik\n",
        "\n",
        "for link in soup.find_all('div'):\n",
        "    cols = link.find_all('span', class_='country-capital')\n",
        "    for col in cols:\n",
        "        capital_name = col.text.strip()\n",
        "        capital_set.add(capital_name)  # Menambahkan nama ibu kota ke dalam set\n",
        "\n",
        "for capital in capital_set:\n",
        "    print(capital)\n"
      ],
      "metadata": {
        "id": "qmZTyzd9UipG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}